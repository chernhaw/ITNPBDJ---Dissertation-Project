{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR3uaJxqD85_",
        "outputId": "5ff91ba9-381a-4ad4-9a93-4aac03adcead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['bak_chor_mee', 'chicken_rice', 'curry_puff', 'fish_soup', 'kaya_toast', 'laksa']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# change the directory in Google Drive for the training data\n",
        "\n",
        "hawker_dir = '/content/drive/My Drive/hawker'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class_labels = os.listdir(hawker_dir)\n",
        "\n",
        "class_labels.sort()\n",
        "\n",
        "\n",
        "print(class_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7E_S8rsLY7k",
        "outputId": "8b3cf925-74e7-4155-c088-f5d40e82aa23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/hawker\n",
            "Found 4788 images belonging to 6 classes.\n",
            "Found 4788 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values between 0 and 1\n",
        "   # featurewise_center=True,\n",
        "    rotation_range=300,  # Randomly rotate images by 20 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift images horizontally by 10%\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically by 10%\n",
        "    shear_range=0.2,  # Apply shear transformation with a shear intensity of 0.2\n",
        "    zoom_range=0.2,  # Randomly zoom images by 20%\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3, #split is 0.3 of the data for verification\n",
        "    fill_mode='nearest'  # Fill any newly created pixels after rotation or shifting\n",
        ")\n",
        "\n",
        "#print(f\"train_hawker_dir:\" +train_hawker_dir)\n",
        "#print(f\"valid_hawker_dir:\" +valid_hawker_dir)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "\n",
        "print(hawker_dir)\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "       # train_hawker_dir,  # This is the source directory for training images\n",
        "        hawker_dir,\n",
        "        target_size=(224, 224),# All images will be resized to 300x300\n",
        "        batch_size=16,\n",
        "        class_mode='categorical'\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "       )\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        #validate_hawker_dir,  # This is the source directory for validation images\n",
        "\n",
        "        hawker_dir,\n",
        "        target_size=(224, 224),# All images will be resized to 300x300\n",
        "        batch_size=16,\n",
        "        class_mode='categorical'\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        )\n",
        "\n",
        "\n",
        "#https://drive.google.com/drive/folders/1a1p31ogK-IspJCOiliX9X9zx9ESwBM5v?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIRPdAJ6g0Ux"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "\n",
        "\n",
        "x= layers.GlobalAveragePooling2D()(pre_trained_model.output)\n",
        "\n",
        "x= layers.Dense(128,activation='relu')(x)\n",
        "\n",
        "# change the drop out\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "# change the regularization weight\n",
        "x=layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "\n",
        "x = layers.Dense  (len(class_labels), activation='softmax')(x)\n",
        "#x=layers.Dense(len(class_labels))(x)\n",
        "\n",
        "model = Model( pre_trained_model.input, x)\n",
        "\n",
        "# Set training parameters\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.0001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu29OchZMN72",
        "outputId": "c9c5f04f-e14c-49e9-ba71-fe10b5be09eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "200/200 [==============================] - 1046s 5s/step - loss: 1.2906 - accuracy: 0.7138 - val_loss: 1.2418 - val_accuracy: 0.8125\n",
            "Epoch 2/25\n",
            "200/200 [==============================] - 668s 3s/step - loss: 0.7351 - accuracy: 0.9115 - val_loss: 1.3511 - val_accuracy: 0.7812\n",
            "Epoch 3/25\n",
            "200/200 [==============================] - 659s 3s/step - loss: 0.6309 - accuracy: 0.9328 - val_loss: 0.9072 - val_accuracy: 0.8438\n",
            "Epoch 4/25\n",
            "200/200 [==============================] - 655s 3s/step - loss: 0.5433 - accuracy: 0.9469 - val_loss: 0.7760 - val_accuracy: 0.8750\n",
            "Epoch 5/25\n",
            "200/200 [==============================] - 649s 3s/step - loss: 0.5052 - accuracy: 0.9583 - val_loss: 0.5223 - val_accuracy: 0.9375\n",
            "Epoch 6/25\n",
            "200/200 [==============================] - 653s 3s/step - loss: 0.4758 - accuracy: 0.9581 - val_loss: 0.3634 - val_accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "200/200 [==============================] - 645s 3s/step - loss: 0.4311 - accuracy: 0.9642 - val_loss: 0.6029 - val_accuracy: 0.8750\n",
            "Epoch 8/25\n",
            "200/200 [==============================] - 653s 3s/step - loss: 0.4154 - accuracy: 0.9641 - val_loss: 0.4841 - val_accuracy: 0.9062\n",
            "Epoch 9/25\n",
            "200/200 [==============================] - 651s 3s/step - loss: 0.3940 - accuracy: 0.9646 - val_loss: 0.2816 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "200/200 [==============================] - 652s 3s/step - loss: 0.3569 - accuracy: 0.9708 - val_loss: 0.3791 - val_accuracy: 0.9688\n",
            "Epoch 11/25\n",
            "200/200 [==============================] - 656s 3s/step - loss: 0.3325 - accuracy: 0.9727 - val_loss: 0.2407 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "200/200 [==============================] - 696s 3s/step - loss: 0.3085 - accuracy: 0.9743 - val_loss: 0.3250 - val_accuracy: 0.9688\n",
            "Epoch 13/25\n",
            " 98/200 [=============>................] - ETA: 5:36 - loss: 0.2773 - accuracy: 0.9833"
          ]
        }
      ],
      "source": [
        "# Constant for epochs\n",
        "EPOCHS = 75\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100, #400 images = batch_size * step 400= 10\n",
        "      epochs=EPOCHS,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=2\n",
        ")\n",
        "\n",
        "# save the model as h5 format\n",
        "\n",
        "model.save('/content/drive/My Drive/models/makan_mobilenet.h5')\n",
        "\n",
        "\n",
        "# plot out the\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the model results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}